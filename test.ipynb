{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(node1, node2):\n",
    "    if isinstance(node1, tuple) and isinstance(node2, tuple):\n",
    "        return np.sqrt((node1[0] - node2[0])**2 + (node1[1] - node2[1])**2)\n",
    "    elif isinstance(node1, tuple) or isinstance(node2, tuple):\n",
    "        point = node1 if isinstance(node1, tuple) else node2\n",
    "        return np.sqrt(point[0]**2 + point[1]**2)\n",
    "    else:\n",
    "        return abs(node1 - node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_association(graph, nodes, edges, threshold):\n",
    "    results = {}\n",
    "    for edge in edges:\n",
    "        associated_nodes = []\n",
    "        for node in nodes:\n",
    "            # Perform edge association based on the given threshold\n",
    "            if distance(node, edge) <= threshold:\n",
    "                # Add node to edge's list of associated nodes\n",
    "                associated_nodes.append(node)\n",
    "        results[edge] = associated_nodes\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_edge_association(graph, nodes, edges, threshold, learning_rate):\n",
    "    results = {}\n",
    "    for edge in edges:\n",
    "        associated_nodes = []\n",
    "        for node in nodes:\n",
    "            # Perform edge association based on the threshold\n",
    "            if distance(node, edge) <= threshold:\n",
    "                # Add node to edge's list of associated nodes\n",
    "                associated_nodes.append(node)\n",
    "        results[edge] = associated_nodes\n",
    "        if len(associated_nodes) == 0:\n",
    "            # Update the threshold based on the learning rate\n",
    "            threshold += learning_rate\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_digital_twin_migration(source_model, target_model, layers_to_transfer, learning_rate):\n",
    "    results = {}\n",
    "    \n",
    "    for layer_name in layers_to_transfer:\n",
    "        if layer_name in source_model and layer_name in target_model:\n",
    "            # Transfer weights from source_model to target_model for the specified layer\n",
    "            source_weights = source_model[layer_name]['weights']\n",
    "            target_model[layer_name]['weights'] = source_weights\n",
    "            results[layer_name] = target_model[layer_name]['weights']\n",
    "        else:\n",
    "            print(f\"Layer '{layer_name}' not found in source_model or target_model.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_initial_state():\n",
    "    return nodes.index(random.choice(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_epsilon_greedy(q_values, state, actions, exploration_rate):\n",
    "    # Ensure that the state index is within the valid range\n",
    "    state = min(max(state, 0), len(q_values) - 1)\n",
    "\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        # Explore: Choose a random action\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # Exploit: Choose the action with the highest Q-value for the current state\n",
    "        action_index = np.argmax(q_values[state])\n",
    "\n",
    "        # Ensure that the action index is within the valid range\n",
    "        action_index = min(max(action_index, 0), len(actions) - 1)\n",
    "        return actions[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(current_state, selected_action):\n",
    "    return current_state + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(current_state, selected_action):\n",
    "    action_value_map = {'action1': 1, 'action2': 2}\n",
    "    \n",
    "    if selected_action in action_value_map:\n",
    "        return current_state + action_value_map[selected_action]\n",
    "    else:\n",
    "        print(f\"Action '{selected_action}' not found in the mapping.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_reinforcement_learning_digital_twin_placement(graph, nodes, edges, actions, rewards, q_values, learning_rate, discount_factor, exploration_rate, max_episodes, max_time_steps):\n",
    "    final_q_values = np.zeros_like(q_values)\n",
    "    for episode in range(max_episodes):\n",
    "        state = select_initial_state()\n",
    "        for time_step in range(max_time_steps):\n",
    "            # Select action using an epsilon-greedy policy\n",
    "            action = select_action_epsilon_greedy(q_values, state, actions, exploration_rate)\n",
    "\n",
    "            # Transition to the next state based on the selected action\n",
    "            next_state = transition(state, action)\n",
    "\n",
    "            # Receive reward based on the transition\n",
    "            reward = get_reward(state, action)\n",
    "\n",
    "            # Ensure that action is always an integer before using it as an index\n",
    "            try:\n",
    "                action = int(action)\n",
    "            except ValueError:\n",
    "                print(f\"Invalid action: {action}. Skipping this time step.\")\n",
    "                continue\n",
    "\n",
    "            # Convert state and action to integers before using them as indices\n",
    "            state = int(state)\n",
    "\n",
    "            # Ensure that next_state is within the valid range of indices\n",
    "            next_state = min(max(next_state, 0), len(q_values) - 1)\n",
    "\n",
    "            # Update Q-values based on the Bellman equation\n",
    "            q_values[state][action] = q_values[state][action] + learning_rate * (reward + discount_factor * np.max(q_values[next_state]) - q_values[state][action])\n",
    "\n",
    "            state = next_state\n",
    "    final_q_values = q_values\n",
    "    return final_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_edge_association(graph, nodes, edges, k):\n",
    "    results = {i: [] for i in range(k)}  # Initialize results with empty lists for each cluster\n",
    "    \n",
    "    # Reshape the nodes to a 2D array\n",
    "    nodes_2d = np.array(nodes).reshape(-1, 1)\n",
    "\n",
    "    # Apply k-means clustering on the nodes\n",
    "    kmeans = KMeans(n_clusters=k).fit(nodes_2d)\n",
    "    node_clusters = kmeans.labels_\n",
    "\n",
    "    # Print clustering results for debugging\n",
    "    print(\"Node Clusters:\", node_clusters)\n",
    "\n",
    "    # Associate edges to nodes in each cluster\n",
    "    for node, cluster in zip(nodes, node_clusters):\n",
    "        print(\"Node:\", node, \"Cluster:\", cluster)\n",
    "        if cluster in results:\n",
    "            if node in graph:\n",
    "                results[cluster].extend(graph[node])\n",
    "            else:\n",
    "                print(f\"Warning: Node {node} not found in the graph.\")\n",
    "        else:\n",
    "            results[cluster] = [graph[node]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_values(threshold, actions):\n",
    "    num_states = 1  # Assuming threshold is a single value\n",
    "    num_actions = len(actions)\n",
    "    q_values = np.zeros((num_states, num_actions))\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_threshold_edges(nodes, edges, action):\n",
    "    updated_nodes = nodes.copy()\n",
    "    updated_edges = edges.copy()\n",
    "    return updated_nodes, updated_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(edges, rewards):\n",
    "    reward = sum(rewards[action]['improvement'] for action in edges)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learning_edge_association(graph, nodes, edges, threshold, learning_rate, exploration_rate, discount_factor, max_episodes):\n",
    "    results = {}\n",
    "    # Initialize Q-values for each state-action pair\n",
    "    q_values = initialize_q_values([threshold], actions)\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state = threshold\n",
    "        for time_step in range(max_time_steps):\n",
    "            # Select action using an epsilon-greedy policy\n",
    "            action = select_action_epsilon_greedy(q_values, state, actions, exploration_rate)\n",
    "\n",
    "            # Update threshold and associated edges based on the selected action\n",
    "            update_threshold_edges(nodes, edges, action)\n",
    "\n",
    "            # Receive reward based on the quality of associations\n",
    "            reward = calculate_reward(edges, rewards)\n",
    "\n",
    "            # Update Q-value for the current state-action pair\n",
    "            next_state = action\n",
    "            q_values[state][action] = q_values[state][action] + learning_rate * (reward + discount_factor * np.max(q_values[next_state]) - q_values[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    results = q_values\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weights(shape):\n",
    "    return np.random.rand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = {}\n",
    "# nodes = [1, 2, 3, 4, 5]\n",
    "# edges = [(1, 2), (2, 3), (3, 4), (4, 5)]\n",
    "# threshold = 1.0\n",
    "# learning_rate = 0.1\n",
    "# input_size = 64\n",
    "# output_size = 32\n",
    "# num_classes = 10\n",
    "# source_model = {\n",
    "#     'layer1': {\n",
    "#         'weights': generate_random_weights((input_size, output_size)),\n",
    "#         'activation': 'relu'\n",
    "#     },\n",
    "#     'layer2': {\n",
    "#         'weights': generate_random_weights((output_size, num_classes)),\n",
    "#         'activation': 'softmax'\n",
    "#     }\n",
    "# }\n",
    "# target_model = {\n",
    "#     'layer1': {\n",
    "#         'weights': generate_random_weights((input_size, output_size)),\n",
    "#         'activation': 'relu'\n",
    "#     },\n",
    "#     'layer2': {\n",
    "#         'weights': generate_random_weights((output_size, num_classes)),\n",
    "#         'activation': 'softmax'\n",
    "#     }\n",
    "# }\n",
    "# layers_to_transfer = ['layer1', 'layer2']\n",
    "# actions = ['action1', 'action2']\n",
    "# rewards = {\n",
    "#     'action1': {\n",
    "#         'improvement': 0.1,\n",
    "#         'consistency': 0.9\n",
    "#     },\n",
    "#     'action2': {\n",
    "#         'improvement': 0.5,\n",
    "#         'consistency': 0.8\n",
    "#     },\n",
    "#     'action3': {\n",
    "#         'improvement': 0.3,\n",
    "#         'consistency': 0.95\n",
    "#     }\n",
    "# } \n",
    "# q_values = np.zeros((len(nodes), len(actions)))\n",
    "# discount_factor = 0.9\n",
    "# exploration_rate = 0.1\n",
    "# max_episodes = 100\n",
    "# max_time_steps = 10\n",
    "# k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = {\n",
    "#     (0, 0): [(1, 0), (0, 1)],\n",
    "#     (1, 0): [(0, 0), (1, 1)],\n",
    "#     (0, 1): [(0, 0), (1, 1)],\n",
    "#     (1, 1): [(0, 1), (1, 0)]\n",
    "# }\n",
    "\n",
    "# nodes = [(0, 0), (1, 0), (0, 1), (1, 1)]\n",
    "\n",
    "# edges = [(0.5, 0.5), (1.5, 0.5), (0.5, 1.5), (1.5, 1.5)]\n",
    "\n",
    "# threshold = 1.0\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# source_model = {\n",
    "#     'layer1': {'weights': [0.1, 0.2, 0.3]},\n",
    "#     'layer2': {'weights': [0.4, 0.5, 0.6]}\n",
    "# }\n",
    "\n",
    "# target_model = {\n",
    "#     'layer1': {'weights': [0.7, 0.8, 0.9]},\n",
    "#     'layer2': {'weights': [1.0, 1.1, 1.2]}\n",
    "# }\n",
    "\n",
    "# layers_to_transfer = ['layer1', 'layer2']\n",
    "\n",
    "# actions = [0, 1, 2]\n",
    "\n",
    "# rewards = {\n",
    "#     0: {'improvement': 0.2},\n",
    "#     1: {'improvement': 0.5},\n",
    "#     2: {'improvement': 0.1}\n",
    "# }\n",
    "\n",
    "# q_values = [[0.0, 0.0, 0.0]]\n",
    "\n",
    "# discount_factor = 0.9\n",
    "# exploration_rate = 0.1\n",
    "# max_episodes = 10\n",
    "# max_time_steps = 5\n",
    "\n",
    "# k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = [(random.uniform(0, 100), random.uniform(0, 100)) for _ in range(50)]\n",
    "# edges = [(random.uniform(0, 100), random.uniform(0, 100)) for _ in range(50)]\n",
    "# graph = {node: [neighbor for neighbor in nodes if np.linalg.norm(np.array(node) - np.array(neighbor)) < 20] for node in nodes}\n",
    "\n",
    "# threshold = 15.0\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# source_model = {\n",
    "#     'layer1': {'weights': np.random.rand(3, 3)},\n",
    "#     'layer2': {'weights': np.random.rand(5, 5)}\n",
    "# }\n",
    "\n",
    "# target_model = {\n",
    "#     'layer1': {'weights': np.random.rand(3, 3)},\n",
    "#     'layer2': {'weights': np.random.rand(5, 5)}\n",
    "# }\n",
    "\n",
    "# layers_to_transfer = ['layer1', 'layer2']\n",
    "\n",
    "# actions = [0, 1, 2]\n",
    "\n",
    "# rewards = {\n",
    "#     0: {'improvement': 0.2},\n",
    "#     1: {'improvement': 0.5},\n",
    "#     2: {'improvement': 0.1}\n",
    "# }\n",
    "\n",
    "# q_values = np.zeros((1, len(actions)))\n",
    "\n",
    "# discount_factor = 0.9\n",
    "# exploration_rate = 0.1\n",
    "# max_episodes = 10\n",
    "# max_time_steps = 5\n",
    "\n",
    "# k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the edge_association function\n",
    "association_results = edge_association(graph, nodes, edges, threshold)\n",
    "for edge, associated_nodes in association_results.items():\n",
    "    print(f\"Associated nodes for edge {edge}: {associated_nodes}\")\n",
    "    for node in nodes:\n",
    "        if distance(node, edge) <= threshold:\n",
    "            print(f\"Node {node} is associated with edge {edge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the adaptive_edge_association function\n",
    "adaptive_results = adaptive_edge_association(graph, nodes, edges, threshold, learning_rate)\n",
    "for edge, associated_nodes in adaptive_results.items():\n",
    "    print(f\"Associated nodes for edge {edge}: {associated_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the transfer_learning_digital_twin_migration function\n",
    "transfer_results = transfer_learning_digital_twin_migration(source_model, target_model, layers_to_transfer, learning_rate)\n",
    "for layer_name, updated_weights in transfer_results.items():\n",
    "    print(f\"Weights for layer {layer_name}: {updated_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the deep_reinforcement_learning_digital_twin_placement function with max_time_steps\n",
    "final_q_values = deep_reinforcement_learning_digital_twin_placement(graph, nodes, edges, actions, rewards, q_values, learning_rate, discount_factor, exploration_rate, max_episodes, max_time_steps)\n",
    "print(\"Final Q-values:\")\n",
    "print(final_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the kmeans_edge_association function\n",
    "association_results = kmeans_edge_association(graph, nodes, edges, k)\n",
    "for node, associated_edges in association_results.items():\n",
    "    print(f\"Associated edges for node {node}: {associated_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the reinforcement_learning_edge_association function\n",
    "q_values = reinforcement_learning_edge_association(graph, nodes, edges, threshold, learning_rate, exploration_rate, discount_factor, max_episodes)\n",
    "print(\"Final Q-values:\")\n",
    "print(q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
